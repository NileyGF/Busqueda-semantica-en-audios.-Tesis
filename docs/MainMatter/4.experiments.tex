%===================================================================================
% Chapter: Experimentation
%===================================================================================
\chapter{Breve Experimentación}
\label{chap:experiment}
%===================================================================================

\section{Consideraciones generales}
\label{sec:consid}
Para evaluar un SRI, tradicionalmente, se parte de un conjunto de documentos y un conjunto de consultas con una relación de relevancia establecida entre ambos \cite{manning2008introductiontoIR}. Esta relación es fundamental para identificar los elementos relevantes para una consulta, e incluso puede determinar el grado de relevancia. Algunos ejemplos de datasets para evaluar SRIs son: \href{https://ir-datasets.com/antique.html}{\textit{antique}}, \href{https://ir-datasets.com/car.html}{\textit{car}}, \href{https://ir-datasets.com/cranfield.html}{\textit{cranfield}}, \href{https://ir-datasets.com/msmarco-passage.html}{\textit{msmarco-passage}}, \href{https://ir-datasets.com/nfcorpus.html}{\textit{nfcorpus}}, \href{https://ir-datasets.com/nyt.html}{\textit{nyt}}.

Las estrategias de evaluación centradas en el usuario buscan tener en cuenta diferentes factores en la percepción de las cualidades musicales, en particular de la similitud musical. Esto es especialmente importante ya que las nociones de similitud musical están probremente definidas. El acuerdo entre los humanos sobre el parecido entre dos piezas musicales está limitado a alrededor del 80\% según se afirma en la literatura \cite{Schedl2014MusicIR}.\\
En general los SRIs evalúan la semejanza utilizando la métrica de similitud de coseno \cite{Brundha2022VectorMB}. Esta técnica calcula la similitud entre dos elementos basándose en el ángulo entre sus representaciones vectoriales. Su popularidad se basa en que es invariante a la magnitud del vector, es eficiente de computar y que en epacios multi-dimensionales captura la orientación de los datos.

Un gran número de estudios de recuperación de información han demostrado que los usuarios de los sistemas de recuperación tienden a prestar atención, principalmente, a los primeros resultados \cite{Mitra2017NeuralMF}. Por lo tanto, las métricas de recuperación de información se centran en comparaciones basadas en los primeros resultados recuperados. Estas métricas generalmente se calculan en una posición, digamos $k$, y luego se promedian sobre todas las consultas.

El hecho de que no existen datasets de recuperación de información de música implica que en todos los experimentos en el tema se definen las consultas y la relevancia entre una consulta y la música de forma diversa.

Utilizando la información proveída por el dataset (sec. \ref{sec:datasets}) fueron concebidos dos conjuntos de consultas. El primero con las descripciones creadas por músicos y el segundo con la lista de aspectos (convertida en oraciones concatenando los \textit{tags}). Durante el preprocesamiento se extraen los \textit{embeddings} de ambos conjuntos (utilizando SBERT \cite{Reimers2019SentenceBERTSE}) y se guardan en archivos binarios. \\
Es necesario establecer una relación de relevancia entre consultas y clips. Para ello fue diseñado un algoritmo que itera por los \textit{embeddings} de cada conjunto de consultas. \\
El algoritmo crea una lista $R$, donde para la consulta i-ésima:
\begin{lstlisting}[language=Python]
R[i] = [(song_id_1, cosine_similarity_i1), ... ,(song-id_j, cosine_similarity_ij)]
\end{lstlisting}
R[i] contiene una tupla por cada consulta $j$ que cumpla: $$cosine\_similarity(i,j) \geq 0.95$$ ya que se considera que a descripciones similares corresponden clips similares. De esta forma siempre se obtiene el clip que describe la consulta $i$ pues $cosine\_similarity(i,i) = 1$.
 
En \textit{Toward Universal Text-To-Music Retrieval} \cite{Doh2022TowardUT} se realiza uno de los análisis más recientes y abarcadores sobre modelos de recuperación de música utilizando texto. Ellos utilizan un subconjunto fijo de 1000 consultas para evaluar varios modelos; por lo que este trabajo considera mantener ese número.

Para ello se extrae un subconjunto aleatorio de 1000 consultas antes de cada proceso de evaluación. Se utilizó un \textit{seed} para que al ejecutar el proceso de evaluación se obtengan resultados consistentes. 

Debido a que fueron propuestos 2 conjuntos de consultas y 3 de descripciones (sec. \ref{subsec:feat-to-text}), cada experimento debe ser repetido 6 veces. El nombre de cada combinación se define como: $consultas-descripciones$ . Más detalles en la figura  \ref{alg:query-corpus}.
\begin{figure}[h!] 
\begin{lstlisting}[language=Python]
descripciones = {corpus: "Descripciones creadas generando oraciones a partir de feature_description y uniendolas en un texto descriptivo.",
                ext_corpus: "Extension del primer corpus; conteniene los mismos textos y las oraciones que lo componen separadas.",
                tag_corpus: "Consiste basicamente en la concatenacion de todos los features que retornan los modelos de la seccion \ref{subsec:essentia} por cada clip."}
consultas = {capt: "Columna \'caption\' del dataset MusicCaps, son descripciones textuales escritas por musicos.",
             aspect: "Esta conformado por la concatenacion de las etiquetas de la columna \'aspect list\' del dataset MusicCaps."}
\end{lstlisting}
\caption{Definición de los conjuntos de descripciones y consultas.} \label{alg:query-corpus}
\end{figure}

\section{Recobrado y precisión}
\label{sec:experiment1}
El primer experimento evalúa el recobrado (\textit{recall}) y la precisión promedio (\textit{mean average precision} mAP). El recobrado representa la proporción entre las canciones recuperadas que son relevantes y el número total de canciones relevantes. En particular se calcula el $recall@k$ con $k=$1, 5, 10, 50; que solo considera los primeros $k$ elementos recuperados. El mAP promedia los resultados de precisión en cada rango donde se encuentra un clip relevante \cite{mAPAnalyticsYogi}. La precisión representa la fracción de elementos recuperados que son relevantes para la consulta del usuario. Esta métrica se ha utilizado durante mucho tiempo como el “estándar de oro” de facto para la evaluación de sistemas de recuperación \cite{Beitzel2009}. Se calcula el mAP general y mAP@10 (donde se consideran solo los top 10 clips recuperados). 

Todas las métricas se calculan para cada consulta y luego se promedian resultando en los valores que se muestran en la tabla \ref{tab:results}.

\begin{table}[h]
    \footnotesize
    \centering
    \begin{tabular} { | l | c | c | c | c | c | c | }
    \hline
                      &   R@1           &   R@5   &   R@10         &   R@50          & mAP@10        &   mAP   \\ 
    \hline
    capt-corpus       & 0.00082         & 0.00499 & 0.0096         & 0.03492         & 0.107         & 0.0352 \\
    capt-ext\_corpus   & 0.00076         & 0.0059  & 0.0095         & 0.03485        & 0.101         & 0.0357  \\ 
    capt-tag\_corpus   & 0.00025         & 0.00202 & 0.0057         & 0.0273         & 0.075         & 0.0374  \\ 
    aspect-corpus     & 0.00124         & 0.0047  & 0.0117         & 0.0424          &\textbf{0.124} & 0.0296 \\ 
    aspect-ext\_corpus &\textbf{0.00137} & 0.0053  & 0.0098         & 0.0344         & 0.078         & 0.028 \\ 
    aspect-tag\_corpus & 0.00056 &\textbf{0.00761} &\textbf{0.01254} &\textbf{0.0524} & 0.096 &\textbf{0.0381} \\ 
    \hline
    \end{tabular}
    \caption{Resultados de las 6 combinaciones consultas-corpus	respecto a recall y mean average precision.}
    \label{tab:results}
\end{table}

Se observa que, en general, utilizar el \textit{aspect list} como consultas provee mejores resultados, en particular cuando se utiliza como corpus los \textit{tags} concatenados. Además el mAP, que se supone que es muy representativo, tiene mejores valores que el recobrado. En cuanto a las consultas que consisten en las descripciones creadas por expertos, los mejores valores se obtienen con el corpus conformado por un texto descriptivo por cada clip. 

También se ilustra que tanto el par de consultas-de-texto-descriptivo, corpus-de-texto-descriptivo; como el par de consultas-de-tags-concatenados, corpus-de-tags-concatenados; resultaron en los mejores valores. Esto puede indicar que es mejor que el corpus a utilizar para describir la música tenga la forma que más prevalece en las consultas de los usuarios. 

\section{Índice de aciertos}
\label{sec:experiment2}

El segundo experimento evalúa una métrica no muy común, pero que refleja de forma simple un factor importante para los usuarios. Fue calculado el índice del modelo de mostrar elementos relevantes en los primeros $k$ clips recuperados (para $k$ que refleja cuantos resultados usualmente comprueban los usuarios (1, 5, 10) ). \\
Entonces el \textit{$hits\_rate@k$} para una consulta tiene como valor 1 si en los primeros $k$ resultados hay algún clip relevante y 0 si no. Finalmente se promedia, para un $k$ fijo, todos los \textit{$hits\_rate@k$}, obteniendo el índice de consultas donde en los top $k$ resultados se recuperan buenos clips.

\begin{table}[h]
    \footnotesize
    \centering
    \begin{tabular} { | l | c | c | c | c | }
    \hline
                       &   hits\_rate@1   &   hits\_rate@5   &  hits\_rate@10 \\ 
    \hline
    capt-corpus        & 0.052           &  0.193          &  \textbf{0.333}  \\
    capt-ext\_corpus   & 0.05            &  0.182          &  0.314          \\ 
    capt-tag\_corpus   & 0.027           &  0.142          &  0.248          \\ 
    aspect-corpus      & \textbf{0.087}  &  \textbf{0.215} &  0.301          \\ 
    aspect-ext\_corpus & 0.052           &  0.137          &  0.209          \\ 
    aspect-tag\_corpus & 0.037           &  0.18           &  0.303          \\ 
    \hline
    \end{tabular}
    \caption{Resultados de las 6 combinaciones consultas-corpus	respecto al índice de aciertos.}
    \label{tab:hitsresults}
\end{table}

En la tabla \ref{tab:hitsresults} se observa que los resultados para esta métrica son bastante uniformes y los mejores valores se obtienen con el corpus número 1. Con la lista de aspectos como consultas y el primer corpus se obtiene casi el doble de resultados relevantes en la primera posición que con el resto. \\
Además, atendiendo a la motivación de este experimento se obtuvo que aproximadamente un tercio de las consultas tienen un clip relevante entre los primeros 10 (de más de 3800 clips). 

\section{Discusión}
\label{sec:final-discution}
Se pudo observar que, en ambos experimentos, no se obtienen resultados muy distintos entre \textit{corpus} y \textit{ext\_corpus}. Lo que conlleva a concluir que del texto se infiere aproximadamente la misma información semántica que de las oraciones que lo componen. El hecho de procesar un corpus 9 veces más grande (en este caso porque solo se utilizaron 9 modelos) aumentó el tiempo de extracción de \textit{embeddings} y el de recuperación considerablemente. Por lo que se debe considerar que no es una buena estrategia aumentar el corpus como se propuso. 

% Al momento de comparar nuestros resultados con los de MuLan \cite{Huang2022MuLanAJ}, MusCALL \cite{Manco2022ContrastiveAL} y Contrastive, Triplet Sentence BERT \cite{Doh2022TowardUT} es muy importante tener en cuenta que la única similitud entre los trabajos es que la tarea es recuperación de música utilizando consultas en lenguaje natural. 
Antes de considerar comparar los resultados obtenidos con otras investigaciones hay que tener en cuenta que ninguna utliza el mismo conjunto de música ni de consultas. Tampoco se comparte la estrategia de recuperación ya que los modelos de MuLan \cite{Huang2022MuLanAJ}, MusCALL \cite{Manco2022ContrastiveAL} y Contrastive Sentence BERT \cite{Doh2022TowardUT} utilizan modelos de aprendizaje contrastivo. Debido a lo novedoso de la estrategia propuesta en este trabajo y al problema que existe en el campo de recuperación de música a partir de texto respecto a la falta de datasets y la ausencia de un protocolo uniforme para la evaluación multimodal, no es factible hacer una comparación que lance alguna conclusión significativa ya que no hay puntos comunes. Sin embargo se espera que, con la publicación de MusicCaps \cite{Agostinelli2023MusicLMGM} y otros datasets en el futuro, surjan nuevas investigaciones o se reevaluen modelos existentes con métricas y datos comunes. 
