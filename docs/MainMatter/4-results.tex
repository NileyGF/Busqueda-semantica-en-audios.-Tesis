\section{Resultados}
\label{sec:results}
Para evaluar SRI, tradicionalmente se parte de un conjunto de documentos y un conjunto de consultas con una relación de relevancia establecida entre ambos conjuntos \cite{manning2008introductiontoIR}. De forma que para una consulta se conoce que documentos son relevantes (y en ocasiones el nivel de relevancia). Algunos ejemplos son: \href{https://ir-datasets.com/antique.html}{\textit{antique}}, \href{https://ir-datasets.com/car.html}{\textit{car}}, \href{https://ir-datasets.com/cranfield.html}{\textit{cranfield}}, \href{https://ir-datasets.com/msmarco-passage.html}{\textit{msmarco-passage}}, \href{https://ir-datasets.com/nfcorpus.html}{\textit{nfcorpus}}, \href{https://ir-datasets.com/nyt.html}{\textit{nyt}}.

Las estrategias de evaluación centradas en el usuario buscan tener en cuenta diferentes factores en la percepción de las cualidades musicales, en particular de la similitud musical. Esto es especialmente importante ya que las nociones de similitud musical están probremente definidas. El acuerdo entre los humanos sobre el parecido entre dos piezas musicales está limitado a alrededor del 80\% según se afirma en la literatura \cite{Schedl2014MusicIR}.
La mayoría de los SRI evalúan semejanza utilizando la similitud de coseno \cite{Brundha2022VectorMB}.\\
Un gran número de estudios de recuperación de información han demostrado que los usuarios de los sistemas de recuperación tienden a prestar atención principalmente a los primeros resultados \cite{Mitra2017NeuralMF}. Por lo tanto, las métricas de recuperación de información se centran en comparaciones basadas en los primeros resultados recuperados. Estas métricas generalmente se calculan en una posición, digamos $k$, y luego se promedian sobre todas las consultas.\\

El hecho de que no existen datasets de recuperación de información de música implica que en todos los experimentos en el tema se definen las consulas y cuáles clips son relevantes a una consulta de forma diversa y un tanto arbitraria.

Utilizando la información que fue proveída en el dataset (sec. \ref{sec:dataset}) fueron concebidos dos conjuntos de consultas. El primero con las descripciones creadas por expertos y el segundo con la lista de aspectos (convertida en oraciones de forma similar al tercer corpus). Durante el preprocesamiento se extraen los embeddings (utilizando SBERT, de igual forma que con los corpus) de ambos conjuntos y se guardan en archivos binarios. Dado que es necesario establecer una relación de relevancia entre consultas y clips, fue diseñado un algoritmo que iterando por todos los embeddings de las consultas (para cada conjunto separadamente), calculando una lista que contenga los índices de los clips, para los cuales la similitud entre las descripciones en el dataset sea mayor o igual que 0.95. De esta forma siempre se obtiene el clip al que pertenece la descripción (pues la similitud es 1) y se debe obtener los clips más similares, en cuanto a como son descritos en MusicCaps.

Con 2 conjuntos de consultas y 3 de corpus hay 6 alternativas. Para cada uno se evaluaron el recobrado promedio (\textit{recall@k} con $k=$1, 5, 1, 50) y el mean average precision (mAP) y el mAP@10. \\
En \cite{Doh2022TowardUT} se realiza uno de los análisis más recientes y abarcadores sobre modelos de recuperación de música utilizando texto. Ellos utilizan un subconjunto fijo de 1000 consultas para evaluar varios modelos; por lo que en este trabajo consideramos mantener ese número para una comparación más consistente.

Para ello se extrae un subconjunto aleatorio de 1000 consultas antes de cada proceso de evaluación. Se utilizó un \textit{seed} para que al ejecutar el proceso de evaluación se obtengan resultados consistentes. En la tabla \ref{tab:results} se puede ver los resultados de las 6 alternativas y 4 resultados de otras investigaciones.

\begin{table}[h]
    \footnotesize
    \centering
    \begin{tabular} { | l | c | c | c | c | c | c | }
    \hline
                      &   R@1           &   R@5   &   R@10          &   R@50          & mAP@10          &   mAP   \\ 
    \hline
    capt-corpus       & 0.00082         & 0.00499 & 0.00964         & 0.03492         & 0.10731         & 0.03522 \\
    capt-ext\_corpus   & 0.00076         & 0.0059  & 0.00947         & 0.03485         & 0.1012          & 0.0357  \\ 
    capt-tag\_corpus   & 0.00025         & 0.00202 & 0.00566         & 0.02725         & 0.07492         & 0.0374  \\ 
    aspect-corpus     & 0.00124         & 0.0047  & 0.01169         & 0.04244         &\textbf{0.12389} & 0.02962 \\ 
    aspect-ext\_corpus &\textbf{0.00137} & 0.0053  & 0.00976         & 0.03436         & 0.07804         & 0.02801 \\ 
    aspect-tag\_corpus & 0.00056 &\textbf{0.00761} &\textbf{0.01254} &\textbf{0.05235} & 0.09573 &\textbf{0.03814} \\ 
    \hline
MuLan \cite{Huang2022MuLanAJ}&    -     &    -    &         -       &        -        &        -        & 0.084   \\ 
MusCALL \cite{Manco2022ContrastiveAL}&  0.259  &  0.519  &  0.633   &        -        &  0.36           &   -     \\ 
Contrastive\_SBERT \cite{Doh2022TowardUT}& 0.068&  0.254  &  0.384   &        -        &  0.15           &   -     \\ 
Triplet\_SBERT \cite{Doh2022TowardUT} &  0.067  &  0.236  &  0.366   &        -        &  0.14           &   -     \\ 
    \hline
    \end{tabular}
    \caption{Resultados de las 6 combinaciones consultas-corpus	respecto a recall y mean average precision. Comparación con las 
    investigaciones relativamente similares.}
    \label{tab:results}
\end{table}

Se observa que en general, utilizar el \textit{aspect list} como consultas provee mejores resultados, en particular cuando se utiliza como corpus los tags concatenados. Además la precisión tiene mejores valores que el recobrado, incluso comparándolos con las otras investigaciones.\\
En cuanto a las consultas de las descripciones creadas por expertos, los mejores valores se obtienen con el corpus conformado por un texto descriptivo por cada clip. El hecho de que tanto el par de las consultas formadas por tags concatenados y el corpus creado de la misma forma (\textit{aspect-tag\_corpus}), como el par de consultas en forma de descripciones y corpus en forma de descripciones, resultaran en los mejores valores; puede indicar que es mejor que el corpus tenga la forma que más prevalece en las consultas de los usuarios o un acercamiento mixto entre descripciones como texto y tags concatenados.

También se pudo observar que no se obtienen resultados muy distintos entre \textit{corpus} y \textit{ext\_corpus}. Lo que conlleva a concluir que del texto se obtiene aproximadamente la misma información semántica que de las oraciones que lo componen. Dado que procesar un corpus 9 veces (en este caso porque solo se utilizaron 9 modelos) más grande aumentó el tiempo de trabajo con BERT y el de recuperación considerablemente; se debe considerar que no es una buena estrategia. 

Al momento de comparar nuestros resultados con los de MuLan \cite{Huang2022MuLanAJ}, MusCALL \cite{Manco2022ContrastiveAL} y Contrastive, Triplet Sentence BERT \cite{Doh2022TowardUT} es muy importante tener en cuenta que la única similitud entre los trabajos es que la tarea es recuperación de música utilizando consultas en lenguaje natural. Ninguna utliza el mismo conjunto de música ni de consultas, tampoco se comparte la estrategia de recuperación ya que ellos tres utilizan modelos de aprendizaje automático (distintos). Además nuestro sistema está utilizando en las consultas vocabulario que no se encuentra en el corpus (debido a que solo se utilizaron 9 modelos dadas las limitaciones de recursos). Si se lograse utilizar TableGPT \cite{Gong2020TableGPTFT} y se incorporan más modelos de clasificación a la extracción de features se debe obtener resultas mejores y posiblementes competitivos con el estado del arte.
