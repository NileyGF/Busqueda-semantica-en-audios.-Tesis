\section{Detalles de implementación}
\label{sec:implementation}

\subsection{Extracción de features}
\label{subsec:essentia}
La arquitectura del sistema de extracción de features fue relativamente sencilla de implementar. Un sistema de clases que heredan de la clase abstracta \textit{FeaturesExtractor}. \\
Las propiedades incluyen: una lista de las etiquetas de clase posibles de cada modelo; \textit{feature\_description} que retorna una oración en lenguaje natural describiendo una etiqueta de clase; \textit{feature\_tags\_description} retorna un tag/metadato describiendo una etiqueta de clase. Además de la función \textit{extract\_feature}, que recibe un clip de música y retorna el(los) feature(s) extraídos con el modelo específico. \\
Los modelos utilizados, para probar y evaluar el diseño son parte de los que provee la biblioteca \textit{essentia} en conjunto con \textit{tensorflow} \cite{alonso2020tensorflow}. Se utilizaron 9 modelos : \href{https://essentia.upf.edu/models.html#genre-discogs400}{Genre Discogs400}, \href{https://essentia.upf.edu/models.html#mtg-jamendo-genre}{MTG-Jamendo genre},  \href{https://essentia.upf.edu/models.html#danceability}{Danceability}, \href{https://essentia.upf.edu/models.html#mood-happy}{Mood Happy}, \href{https://essentia.upf.edu/models.html#mood-relaxed}{Mood Relaxed}, \href{https://essentia.upf.edu/models.html#mood-sad}{Mood Sad}, \href{https://essentia.upf.edu/models.html#mtg-jamendo-mood-and-theme}{MTG-Jamendo mood and theme}, \href{https://essentia.upf.edu/models.html#mtg-jamendo-instrument}{MTG-Jamendo instrument} y \href{https://essentia.upf.edu/models.html#voice-gender}{Voice gender}.

Los modelos de clasificación requiren que el audio sea convertido a embeddings, para recibirlos como entrada, y fue utilizado para ello: \href{https://essentia.upf.edu/models.html#discogs-effnet}{\textit{Discogs-EffNet}}.\\
En \href{https://github.com/NileyGF/Busqueda-semantica-en-audios.-Tesis/blob/main/src/data/musiccaps-subset-feat.csv}{\textbf{musiccaps-subset-feat.csv}} se puede observar el subconjunto utilizado de MusicCaps con los features extraídos, además de la información originaria del dataset sobre 
cada clip. Este proceso, en una computadora con 24 GB de RAM y procesador Intel Core i7-4710HQ, tomó alrededor de 18 horas para terminar con los alrededor de 3800 clips.

\subsection{Conversión de tags en una descripción en lenguaje natural}
\label{subsec:feat-to-text}
Como fue mencionado en la sección \ref{sec:design}, se intentó implementar tres alternativas para obtener descripciones a partir de los features extraídos anteriormente. \\

\textbf{Utilizar GPT-2 con \textit{prompt engineering}}\\
Se intentó unas decenas de prompts utilizando el modelo de completamiento de texto de GPT-2. Sin embargo, no se obtuvo resultados aceptables con ningún de ellos. Teniendo en cuenta dichos resultados se intentó con GPT-3.5, desde la página de Open-AI; y esta vez si se observaron buenas descripciones, aunque no completamente fieles, como es de esperar de estos modelos. Sin embargo, no es factible realizar casi 4000 peticiones, ya que a diferencia de GTP-2, GPT-3.5 no es tan accesible. Por lo tanto esta alternativa tuvo que ser descartada.\\

\textbf{Crear una oración con fuerza bruta}\\
Utilizando la propiedad feature\_description, se generó una oración por cada feature extraído, por cada uno de los clips en el dataset. La concatenación de dichas oraciones en un texto constituyó el corpus de descripciones. \\
Ejemplos de la metodología para crear las oraciones:
\begin{itemize}
    \item MTG-Jamendo genre: `` The music genre sounds like \{\textit{genre classification}\}. ''
    \item Mood Relaxed: `` Its sound is \{\textit{relaxing/not relaxing}\}. ''
    \item MTG-Jamendo instrument: `` You can hear the sounds of \{\textit{comma separated top instruments}\}. ''
    \item Voice gender: `` There is a \{\textit{female/male}\} voice. ''
\end{itemize}

Aprovechando la libertad al diseñar el sistema de recomendación se propuso realizar un segundo corpus de descripciones conteniendo las oraciones separadas además del texto resultado de la concatenación. De forma que este corpus extendido tiene 10 descripciones (que posteriormente son transformadas en embeddings) por cada clip (una oración por cada uno de los 9 features y el texto completo). Esta idea surge para comparar que ofrece mejores resultados en la recuperación con embeddings: una consulta en forma de texto largo u oraciones más cortas, pero repetitivas a través del conjunto de datos.\\

\textbf{Concatenar features sin intentar que parezca una oración humana}\\
Utilizando la propiedad \textit{feature\_tags\_description}, se creó una `oración' por cada clip, donde el valor de cada feature es separado del siguiente por un `; '. En algunos, la propiedad modifica ligeramente el valor resultante del modelo de clasificación. Por ejemplo, tiene más sentido semántico utilizar \textit{``\{female/male\} voice ''}, que simplemente \textit{``\{female/male\}''}.\\

En \href{https://github.com/NileyGF/Busqueda-semantica-en-audios.-Tesis/blob/main/src/data/musiccaps-subset-descriptions.csv}{\textbf{musiccaps-subset-descriptions.csv}} se pueden observar las descripciones de cada clip, tanto en formato de oraciones procesadas, como de tags concatenados. 

\subsection{Extracción de embeddings de BERT}
\label{subsec:bert-embedd-extr}
Cada uno de los 3 corpus de descripciones fueron posteriormente procesados para crear un vector de embeddings utilizando el modelo de la biblioteca \textit{transformers} de \textit{HuggingFace}: \textbf{bert-base-uncased}. El proceso consiste en tokenizar (con la función BertTokenizer de 
\textit{transformers}) el texto: 
\begin{lstlisting}
preprocess_tokens = `[CLS]' + text + `[SEP]'
\end{lstlisting} 
Luego, utilizando la biblioteca \textit{torch} y el modelo de BERT se transforma el texto tokenizado en embeddings. Al finalizar se guarda como un archivo binario la lista de embeddings, que no se encuentran en el repositorio, debido a que ocupan relativamente bastante espacio.

El primer corpus de embeddings tomó en computarse, en una computadora con 24 GB de RAM y procesador Intel Core i7-4710HQ, alrededor de 2 horas. El segundo, que consiste en una extensión del primero con 10 veces la cantidad de embeddings, tomó entre 14 y 15 horas de procesamiento. El tercer corpus, 
constituido por la concatenación de tags tomó entre 1 y 2 horas en terminar.

Como último detalle, en esta parte de la implementación, al finalizar de procesar cada corpus, se creó un diccionario que establece por cada embedding a que clip corresponde. Esto es particularmente importante en el caso del segundo, ya que existen 10 embeddings por cada clip.

\subsection{Recuperación de Información. Comparación y ranking}
\label{subsec:IRS-process-query}
El método de obtener la música más cercana, dado una consulta, consiste en obtener un vector de embeddings, utilizando el mismo proceso descrito anteriormente en  \ref{subsec:bert-embedd-extr}, luego se compara dicho vector con cada uno de los embeddings en el corpus (solo se emplea uno de los tres), utilizando la similitud del coseno entre el ángulo de los vectores. Finalmente se ordenan descendentemente, ya que el coseno es mayor entre vectores más cercanos. Se puede especificar cuantos resultados obtener: todos, o solo los $k$ más similares a la consulta. 

Después de ordenar los embeddings se busca la correspondencia entre ellos y los clips, de forma que la relevancia de un clip es la relevancia del primer embedding correspondiente a él (o sea, el de mayor valor de coseno). Esto es particularmente importante para el segundo corpus y cualquier otro que se pueda diseñar eventualmente donde a un clip le corresponda más de un vector de embeddings (por ejemplo, si la descripción contiene más de 512 tokens no puede ser procesada con BERT, y puede solucionarse dividiéndola en varias subdescripciones más cortas).
